{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-custody",
   "metadata": {},
   "source": [
    "# IPAI Homework 2. English 2 Lark translator\n",
    "\n",
    "#### Work done by Pavel Tishkin, p.tishkin@innpolis.university"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-hawaiian",
   "metadata": {},
   "source": [
    "## Installing the nesessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lark-parser\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-impact",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reflected-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://www.nltk.org/book/ch03.html\n",
    "# https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "# Information Retreival cource, Lab with stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-stanley",
   "metadata": {},
   "source": [
    "## Importing nesessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "weekly-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-header",
   "metadata": {},
   "source": [
    "## Defining EBNF structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "entitled-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2lark_parser = Lark(r\"\"\"\n",
    "    ?value: deftemplate\n",
    "          | assert\n",
    "          | defrule\n",
    "    \n",
    "    assert: \"there exist\" template (property)+\n",
    "    deftemplate: template \"templat properti\" (propertyname)+\n",
    "    defrule: \"if\" assert \"then\" assert\n",
    "    property: propertyname propertyvalue\n",
    "    template: WORD\n",
    "    propertyname: WORD\n",
    "    propertyvalue: WORD\n",
    "    \n",
    "    %import common.WORD\n",
    "    %ignore \" \"\n",
    "\"\"\", start='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-wisconsin",
   "metadata": {},
   "source": [
    "## Parsers for the BNF structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "independent-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original(word, original_tokens):\n",
    "    return list(filter(lambda x: word in x.lower(), original_tokens)).pop()\n",
    "\n",
    "def slot_to_str(slot):\n",
    "    return f'(slot, {slot})'\n",
    "\n",
    "def parse_deftemplate(tree, original_tokens):\n",
    "    template_stemmed = tree.children[0].children[0].lower()\n",
    "    template_name = get_original(template_stemmed, original_tokens).lower()\n",
    "    template_properties = [get_original(w.children[0].lower(), original_tokens).lower() for w in tree.children[1:]]\n",
    "    result = f'(deftemplate {template_name}\\n  '\n",
    "    for p in template_properties:\n",
    "        result += f' (slot {p})'\n",
    "    return result + ')', {template_name: template_properties}\n",
    "\n",
    "def parse_assert(tree, original_tokens, models=None, add_assert=True):\n",
    "    # Getting name of the model from original tokens\n",
    "    template_stemmed = tree.children[0].children[0].lower()\n",
    "    template_name = get_original(template_stemmed, original_tokens).lower()\n",
    "    # A simple flag for defrules. One of its asserts does not need \"(assert )\" in it.\n",
    "    if add_assert:\n",
    "        result = f'(assert ({template_name}'\n",
    "    else:\n",
    "        result = f'({template_name}'\n",
    "    template_properties = [(p.children[0].children[0].lower(), p.children[1].children[0].lower()) for p in tree.children[1:]]\n",
    "    # Idea is the following. We take values from the original sentence, slot names are either taken stemmed\n",
    "    # Or they are taken from the model slot names if such model exists\n",
    "    for i in range(len(template_properties)):\n",
    "        # Getting value from the original\n",
    "        name = get_original(template_properties[i][1], original_tokens)\n",
    "        # Getting slot stemmed\n",
    "        slot = template_properties[i][0]\n",
    "        # Or getting it from the existing model\n",
    "        if models != None:\n",
    "            if template_name in models.keys():\n",
    "                model = models[template_name]\n",
    "                for j in range(len(model)):\n",
    "                    if slot in model[j]:\n",
    "                        slot = model[j]\n",
    "        result += f' ({slot} \"{name}\")'\n",
    "    # Additional parenthesies for \"(assert )\"\n",
    "    if add_assert:\n",
    "        result += ')'\n",
    "    return result + ')'\n",
    "\n",
    "def parse_defrule(tree, original_tokens, rulename, models=None):\n",
    "    # Defrule is a simple combination of two asserts.\n",
    "    return f'(defrule\\n   {rulename} {parse_assert(tree.children[0], original_tokens, models, add_assert=False)} => {parse_assert(tree.children[1], original_tokens, models)})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-failing",
   "metadata": {},
   "source": [
    "## Reading input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-essence",
   "metadata": {},
   "source": [
    "Input should be typed from the 'input.txt'. Each new sentence should start from the new line. The result is going to be in the file output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "white-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ''\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "with open('input.txt', 'r') as inp:\n",
    "    with open('output.txt', 'w+') as out:\n",
    "        text = inp.readlines()\n",
    "        models = {}\n",
    "        for i in range(len(text)):\n",
    "            sentence = text[i]\n",
    "            # Tokenizing and getting rid of punctuation\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "            # Lowercasing\n",
    "            lowercase = [w.lower() for w in tokenized_sentence]\n",
    "            # Getting rid of stopwords. Stopwords IF, THERE, THEN are reserved by BNF for defining assertions and defrules.\n",
    "            used_stopwords = list(filter(lambda word: (word != 'if' and word != 'there' and word != 'then'), stopwords.words('english')))\n",
    "            filtered = list(filter(lambda word: word not in used_stopwords , lowercase))\n",
    "            # Stemming words. Useful when verbs are used instead of nouns for asserts.\n",
    "            stemmed = [stemmer.stem(w) for w in filtered]\n",
    "            filtered_sentence = \" \".join(stemmed)\n",
    "            try:\n",
    "                sent_tree = eng2lark_parser.parse(filtered_sentence)\n",
    "                if sent_tree.data == 'deftemplate':\n",
    "                    res, model = parse_deftemplate(sent_tree, tokenized_sentence)\n",
    "                    out.write(res + '\\n')\n",
    "                for k in model.keys():\n",
    "                    models[k] = model[k]\n",
    "                if sent_tree.data == 'assert':\n",
    "                    out.write(parse_assert(sent_tree, tokenized_sentence, models) + '\\n')\n",
    "                if sent_tree.data == 'defrule':\n",
    "                    out.write(parse_defrule(sent_tree, tokenized_sentence, f'rule{i}', models) + '\\n')\n",
    "            except Exception:\n",
    "                out.write(f'Error in line {i+1}. Failed to parse sentences!\\n')                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-missile",
   "metadata": {},
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-hormone",
   "metadata": {},
   "source": [
    "1. Somewhat models the behaviour of the language\n",
    "2. If the model was defined previously, the program will get the slot names from the already defined model\n",
    "3. Can work out cases when the slot name is given in the form of verb (named -> name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-links",
   "metadata": {},
   "source": [
    "## Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-guitar",
   "metadata": {},
   "source": [
    "1. Can not work out the cases where (slotname, value) are swapped to (value, slotname)\n",
    "2. Generally limited to the syntax of the system\n",
    "3. If the model was not defined, some of the inferences would not be correct due to stemming (figure -> figur, for instance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
